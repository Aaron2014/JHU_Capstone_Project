---
title: "SwiftKey_Project_Week 4 (Task 5)"
author: "Wanjie Feng"
date: "2024-04-016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Project Overview


In previous project, a 1-gram, 2-gram and 3-gram were used to build the model and calculate the probability of a next-word prediction. In this project, I increased the grams to 2-gram, 3-gram, and 4-gram. This should increase the accuracy of the model. The content of this project is:

* Read in n-gram data
* N-grams Preparation
* Build the model
* Summary


## 1. Read in Data


```{r, warning=FALSE, message=FALSE}
start <- Sys.time()
library(stringr)
library(stringi)
library(qdap)
library(quanteda)
library(data.table)
gram_1 <- read.csv("gram_2.csv")
gram_2 <- read.csv("gram_3.csv")
gram_3 <- read.csv("gram_4.csv")
gram_1 <- data.table(gram_1)
gram_2 <- data.table(gram_2)
gram_3 <- data.table(gram_3)
gram_1[,c("X"):=NULL]
gram_2[,c("X"):=NULL]
gram_3[,c("X"):=NULL]
```

## 2. $N$-grams Preparation

The Katz backoff model requires several sets of $N$-gram and ($N$-1)-gram data, according to the user input, to successfully calcualte all the necessary probabilities for comparisons and choose the model suitable next word candidate. The detail algorithm can be found from the two links above.


```{r}
##Return all the observed N-grams given the previous (N-1)-gram
##
## - wordseq: character vector of (N-1)-gram separated by underscore, e.g. "x1_x2_..._x(N-1)"
## - Ngram: datatable of N-grams
get_obs_ngram <- function(wordseq, Ngram){
  PreTxt <- sprintf("%s%s%s", "^", wordseq, "_")
  quest <- grep(PreTxt, Ngram$Word, perl=T, useBytes = T)
  Ngram[quest,]
}

get_unobs_ngram_tails <- function(obsngram, N){
  obstails <- str_split_fixed(obsngram$Word, "_", N)[,N]
  return(data.table(Word=gram_1[!obstails,Word,on="Word"]))
}

cal_obs_prob <- function(ObsNgrams, Nm1Grams, wordseq) {
  PreCount <- Nm1Grams[wordseq, Counts, on=.(Word)]
  ObsNgrams[,Prob:=ObsNgrams[,cDis]/PreCount]  # c_dis/c
}

cal_alpha <- function(ObsNGrams, Nm1Grams, wordseq) {
  if (dim(ObsNGrams)[1] != 0) {
    # return(1-sum(ObsNGrams[,.(Qbo)]))  # We don't use this formular because End Of Sentence is not counted
    return(sum(ObsNGrams[,Counts-cDis]/Nm1Grams[wordseq, Counts, on=.(Word)]))
  } else {
    return(1)
  }
}

```



## 3. Build the model

The final part is to integrate the above steps. 

```{r}
## Find next word. 
## Return a list of predicted next words according to previous 2 user input words

## - xyz:  string of user-input bigram, separated by a space
## - predict_n: number of words to predict
Find_Next_word  <- function(xyz, words_num){
  xyz <- gsub(" ", "_", xyz)
  if (length(which(gram_2$Word == xyz)) > 0) {  # C(x,y) > 0
    ## N-grams preparation
    # Retrieve all observed trigrams beginning with xy: OT
    ObsTriG <- get_obs_ngram(xyz, gram_3)
    yz <- str_split_fixed(xyz,"_", 2)[,2]
    # Retrieve all observed bigrams beginning with y: OB
    ObsBiG <- get_obs_ngram(yz, gram_2)
    # Retrieve all unigrams end the unobserved bigrams UOBT: z where C(y,z) = 0, UOB in UOT
    UnObsBiTails <- get_unobs_ngram_tails(ObsBiG, 2)
    # Exclude observed bigrams that also appear in observed trigrams: OB in UOT
    ObsBiG <- ObsBiG[!str_split_fixed(ObsTriG[,Word], "_", 2)[,2], on="Word"]

    ## Calculation part
    # Calculate probabilities of all observed trigrams: P^*(z|x,y)
    ObsTriG <- cal_obs_prob(ObsTriG, gram_2, xyz)
    # Calculate Alpha(x,y)
    Alpha_xy <- cal_alpha(ObsTriG, gram_2, xyz)
    # Calculate probabilities of all observed bigrams: P^*(z|y), (y,z) in UOT
    ObsBiG <- cal_obs_prob(ObsBiG, gram_1, yz)
    # Calculate Alpha(y)
    Alpha_y <- cal_alpha(ObsBiG, gram_1, yz)
    # Calculate P_{ML}(z), where c(y,z) in UOB: Alpha_y * P_{ML}(z)
    UnObsBiTails[, Prob:=gram_1[UnObsBiTails, Counts, on=.(Word)]/gram_1[UnObsBiTails, sum(Counts), on=.(Word)]]
    UnObsBiTails[, Prob:=Alpha_xy*Alpha_y*Prob]
    # Remove unused column in ObsTriG and ObsBiG
    ObsTriG[, c("Counts", "cDis"):=NULL]
    ObsBiG[, c("Counts", "cDis"):=NULL]
    #remove xy, only keep predict word
    ObsTriG[, Word:=str_remove(ObsTriG[, Word], "([^_]+_)+")]
    #remove y, only keep predict word
    ObsBiG[, Word:=str_remove(ObsBiG[, Word], "([^_]+_)+")]
    # Compare OT, Alpha_xy * P_{Katz}(z|y)
    # P_{Katz}(z|y) = 1. P^*(z|y), 2. Alpha_y * P_{ML}(z)
    ObsBiG[,Prob:=Alpha_xy*Prob]
    AllTriG <- setorder(rbind(ObsTriG, ObsBiG, UnObsBiTails), -Prob)
    return(AllTriG[Prob!=0][1:min(dim(AllTriG[Prob!=0])[1], words_num)])
  } else {  # C(x,y) = 0, or xy is not in N-gram, goes down to (N-1)-gram
    yz <- str_split_fixed(xyz,"_", 2)[,2]
    # c(y>0)
    if (length(which(gram_1$Word == yz)) > 0) {
      # Retrieve all observed bigrams beginning with y: OB
      ObsBiG <- get_obs_ngram(yz, gram_2)
      # Calculate probabilities of all observed bigrams: P^*(z|y)
      ObsBiG <- cal_obs_prob(ObsBiG, gram_1, yz)
      # Calculate Alpha(y)
      Alpha_y <- cal_alpha(ObsBiG, gram_1, yz)
      # Retrieve all unigrams end the unobserved bigrams UOBT: z where C(y,z) = 0
      UnObsBiTails <- get_unobs_ngram_tails(ObsBiG, 2)
      # Calculate P_{ML}(z), where c(y,z) in UOB: Alpha_y * P_{ML}(z)
      UnObsBiTails[, Prob:=gram_1[UnObsBiTails, Counts, on=.(Word)]/gram_1[UnObsBiTails, sum(Counts), on=.(Word)]]
      UnObsBiTails[, Prob:=Alpha_y*Prob]
      # Remove unused column in ObsBiG
      ObsBiG[, c("Counts", "cDis"):=NULL]
      ObsBiG[, Word:=str_remove(ObsBiG[, Word], "([^_]+_)+")]
      AllBiG <- setorder(rbind(ObsBiG, UnObsBiTails), -Prob)
      return(AllBiG[Prob!=0][1:words_num])
    } else {  # c(y=0) or y is not in (N-1)-gram, goes down to (N-2)-gram
      # P^*z
      result <- setorder(gram_1, -cDis)[1:words_num,.(Word, Prob=cDis/gram_1[,sum(Counts)])]
      result$Word <- str_replace(result$Word,"_", " ")
      return(result)  
    }
  }
}

```

Process the input phrase. Get the last **three** words as input to the model. 

```{r}
## Remove elements not being used by prediction model
pre_precess <- function(wordseq){
  tok <- tokens(char_tolower(wordseq))
  tok <- tokens(tok, remove_punct = TRUE)
  tok <- tokens(tok, remove_numbers = TRUE)
  tok <- tokens(tok, remove_symbols = TRUE)
  tok <- tokens(tok, remove_url = TRUE)
  tok <- tokens(tok, remove_HashTags = TRUE)
  tok <- tokens(tok, remove_TwitterHandles = TRUE)
  tok <- tokens(tok, remove_nonAscii = TRUE)
  return(paste(tail(tok[[1]], 3), collapse = " "))
}
```

Predict next word. Here predict 5 possible words listed high to low by their probablities. 

```{r}
Next_word_pred <- function(prephrase, num=5){
  temp <- pre_precess(prephrase)
  result <- Find_Next_word(temp, num)
  if (dim(result)[1]==0){
    rbind(result, list("<Please input more words",1))
  }
  return(result)
}
```

Test models using few phrases

```{r}
Next_word_pred("He likes to eat ice")
```

```{r}
Next_word_pred("the prime minister")
```

```{r}
Next_word_pred("a nuclear power")
```
Predict more than 1 word

```{r}
#add predicted word to input phrase
add_word <- function(prephrase, pred_word){
  new_phrase <- paste(prephrase, pred_word)
  return(new_phrase)
}

```

Here assumes 5 words to predict. 
```{r}
Next_words_pred <- function(prephrase, num=5){
  results <- data.frame()
  for (I in 1:num){
    temp <- pre_precess(prephrase)
    next_word <- Find_Next_word(temp, 1)
    if (dim(next_word)[1]==0){
      rbind(next_word, list("<Please input more words",1))
    }
    prephrase<- add_word(prephrase, next_word$Word)
    results <- rbind(results, next_word)
  }
  # here assume the simplest probabilities. Each prediction is independent.
  for (I in 2:num){
    results[I,]$Prob <- results[I,]$Prob*results[I-1,]$Prob
  }
  output <- list(results, prephrase)
  return(output)
}

```


```{r}
output <- Next_words_pred("He likes to eat ice")
print(output[[1]])
```
print the predicted sentences.

```{r}
print(output[[2]])
```
It is clear that the first word has higher probabilities and the probability of next word is lower. With sentence longer and longer, the probability or perplexity becomes lower and lower. 

## 4. Summary

It looks like using 2-gram, 3-gram and 4-gram doesn't increase the accuracy too much. The prediction of next word sometimes doesn't make sense. 


