{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afba8e0-73d7-45b2-aeb1-ecef38f1fcf4",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"5\">Predict Next Word</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f9e02-57be-4e5c-9b73-38d9cfb0bec9",
   "metadata": {},
   "source": [
    "<h2>Project Overview</h2>\n",
    "\n",
    "__This project is a course project from Data Science: Statistics and Machine Learning Specialization. This course is taught by Johns Hopkins University__ \n",
    "\n",
    "This project is part of the capstone project of this course. Specifically, this project is for Week 4 (Task 5). A LSTM model is built to predict the next word. \n",
    "\n",
    "This project refers https://www.geeksforgeeks.org/next-word-prediction-with-deep-learning-in-nlp/\n",
    "\n",
    "Implementing these codes in R is more complicated, since it uses TensorFlow and Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a983b3-a94f-4a8a-8aba-ef0c6a47a9e5",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<ol>\n",
    "    <li><a href=\"#1\">Read in Data</a></li>\n",
    "    <li><a href=\"#2\">Preprocessing the dataset</a></li>\n",
    "    <li><a href=\"#3\">Build the Model</a></li>\n",
    "    <li><a href=\"#4\">Train the Model</a></li>\n",
    "    <li><a href=\"#5\">Predict the next word</a></li>\n",
    "</ol>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06332a7-3ea8-4844-bbb2-bfb819bb02bf",
   "metadata": {},
   "source": [
    "## 1. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831ea92a-2c73-4f2f-8d01-685965255e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np \n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682b55b6-bc0a-4ee4-ba6f-a89ae9dba6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_sentence_list(file_path): \n",
    "    with open(file_path, 'r',encoding='UTF-8') as file: \n",
    "        text = file.read().lower() \n",
    "  \n",
    "    # Splitting the text into sentences using \n",
    "    # delimiters like '.', '?', and '!' \n",
    "    sentences = [sentence.strip() for sentence in re.split( \n",
    "        r'(?<=[.!?])\\s+', text) if sentence.strip()] \n",
    "  \n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a3a83c-b048-46e1-8107-d1f8be6d3f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.2 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_path = \"D:/Core/Google/Data Science--Statistics and ML/5. Capstone/final/en_US/en_US.blogs.txt\"\n",
    "blog_data = file_to_sentence_list(file_path)\n",
    "file_path = \"D:/Core/Google/Data Science--Statistics and ML/5. Capstone/final/en_US/en_US.news.txt\"\n",
    "news_data = file_to_sentence_list(file_path) \n",
    "file_path = \"D:/Core/Google/Data Science--Statistics and ML/5. Capstone/final/en_US/en_US.twitter.txt\"\n",
    "twitter_data = file_to_sentence_list(file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b50462b-cf3d-4aad-838d-f88da19fed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lenght of blog data is  2092948\n",
      "Total lenght of blog data is  1842069\n",
      "Total lenght of blog data is  2926549\n"
     ]
    }
   ],
   "source": [
    "print(\"Total lenght of blog data is \", len(blog_data))\n",
    "print(\"Total lenght of blog data is \", len(news_data))\n",
    "print(\"Total lenght of blog data is \", len(twitter_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665d1a7-4c58-42b0-90cc-f048772f88dd",
   "metadata": {},
   "source": [
    "## 2. Preprocss the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa47302-4fe5-4e89-a329-b93a84a02aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of data is  6861566\n"
     ]
    }
   ],
   "source": [
    "# combine all three datasets\n",
    "text_data = blog_data + news_data + twitter_data\n",
    "print(\"The total length of data is \", len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2aca069-ce83-4f7d-89c5-203a8f5290cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_list_by_percentage(list, rate):\n",
    "  \"\"\"Samples a list by percentage.\n",
    "\n",
    "  Args:\n",
    "    list: The list to sample.\n",
    "    percentage: The percentage of the list to sample.\n",
    "\n",
    "  Returns:\n",
    "    A list of the sampled elements.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate the number of elements to sample.\n",
    "  num_elements_to_sample = int(len(list) * rate)\n",
    "\n",
    "  # Sample the list.\n",
    "  sampled_list = random.sample(list, num_elements_to_sample)\n",
    "\n",
    "  # Return the sampled list.\n",
    "  return sampled_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ecf49-fc08-41b0-93c4-7aaab65092c6",
   "metadata": {},
   "source": [
    "The entire data set is too big. It takes a long time to train the model. Here uses part of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0beff6a-180a-470e-8954-81c5f79178a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of data after sampling is  6861\n"
     ]
    }
   ],
   "source": [
    "# sample the data\n",
    "sample_size = 0.001\n",
    "text_data = sample_list_by_percentage(text_data, sample_size)\n",
    "print(\"The total length of data after sampling is \", len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b655b93-7579-482a-a2af-a4e69f82d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 266 ms\n",
      "Wall time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text_data) \n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e15e1db-c5d7-4c0e-bbde-8c062d3f0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences \n",
    "input_sequences = [] \n",
    "for line in text_data: \n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] \n",
    "    for i in range(1, len(token_list)): \n",
    "        n_gram_sequence = token_list[:i+1] \n",
    "        input_sequences.append(n_gram_sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5161a3-d6be-4494-a9f5-c9e96bb42d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 547 ms\n",
      "Wall time: 776 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Pad sequences and split into predictors and label \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences]) \n",
    "input_sequences = np.array(pad_sequences( \n",
    "    input_sequences, maxlen=max_sequence_len, padding='pre')) \n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1] \n",
    "  \n",
    "# Convert target data to one-hot encoding \n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130e5d1-2d11-4bef-bdbb-7879e28fd2ba",
   "metadata": {},
   "source": [
    "## 3. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb616bb-3563-46d9-96f6-2beb550d8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 547 ms\n",
      "Wall time: 732 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the model \n",
    "model = Sequential() \n",
    "model.add(Embedding(total_words, 10, \n",
    "                    input_length=max_sequence_len-1)) \n",
    "model.add(LSTM(128)) \n",
    "model.add(Dense(total_words, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d362e-437f-4398-b75d-46c5f54cd3c5",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9aa4de-a2ce-4a46-8c9f-9da59b2aec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3001/3001 [==============================] - 363s 120ms/step - loss: 7.6263 - accuracy: 0.0502\n",
      "Epoch 2/10\n",
      "3001/3001 [==============================] - 345s 115ms/step - loss: 7.1120 - accuracy: 0.0627\n",
      "Epoch 3/10\n",
      "3001/3001 [==============================] - 379s 126ms/step - loss: 6.7842 - accuracy: 0.0757\n",
      "Epoch 4/10\n",
      "3001/3001 [==============================] - 392s 131ms/step - loss: 6.4826 - accuracy: 0.0877\n",
      "Epoch 5/10\n",
      "3001/3001 [==============================] - 395s 132ms/step - loss: 6.1786 - accuracy: 0.0956\n",
      "Epoch 6/10\n",
      "3001/3001 [==============================] - 355s 118ms/step - loss: 5.8705 - accuracy: 0.1038\n",
      "Epoch 7/10\n",
      "3001/3001 [==============================] - 323s 108ms/step - loss: 5.5671 - accuracy: 0.1118\n",
      "Epoch 8/10\n",
      "3001/3001 [==============================] - 323s 108ms/step - loss: 5.2710 - accuracy: 0.1260\n",
      "Epoch 9/10\n",
      "3001/3001 [==============================] - 322s 107ms/step - loss: 4.9897 - accuracy: 0.1467\n",
      "Epoch 10/10\n",
      "3001/3001 [==============================] - 323s 108ms/step - loss: 4.7263 - accuracy: 0.1718\n",
      "CPU times: total: 1h 27min 50s\n",
      "Wall time: 58min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f92c6d0d60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model \n",
    "model.fit(X, y, epochs=10, verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1fbd2-6f1a-44d1-a64a-ad9acdc83185",
   "metadata": {},
   "source": [
    "## 5. Predict Next Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7199eb4a-2714-40b8-bcd1-0f89a3c26b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 790ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Next predicted words: When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd have a good bit but\n"
     ]
    }
   ],
   "source": [
    "# Generate next word predictions \n",
    "seed_text = \"When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd\"\n",
    "next_words = 5\n",
    "  \n",
    "for _ in range(next_words): \n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "    token_list = pad_sequences( \n",
    "        [token_list], maxlen=max_sequence_len-1, padding='pre') \n",
    "    predicted_probs = model.predict(token_list) \n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)] \n",
    "    seed_text += \" \" + predicted_word \n",
    "  \n",
    "print(\"Next predicted words:\", seed_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b571739d-5729-489f-9fa1-b346c1772349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Next predicted words: He likes to eat in the meantime of the\n"
     ]
    }
   ],
   "source": [
    "# Generate next word predictions \n",
    "seed_text = \"He likes to eat\"\n",
    "next_words = 5\n",
    "  \n",
    "for _ in range(next_words): \n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "    token_list = pad_sequences( \n",
    "        [token_list], maxlen=max_sequence_len-1, padding='pre') \n",
    "    predicted_probs = model.predict(token_list) \n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)] \n",
    "    seed_text += \" \" + predicted_word \n",
    "  \n",
    "print(\"Next predicted words:\", seed_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81f632f-33ad-4828-a86d-b6c180cc7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Next predicted words: The prime minister twin has crawled the legislation\n"
     ]
    }
   ],
   "source": [
    "# Generate next word predictions \n",
    "seed_text = \"The prime minister\"\n",
    "next_words = 5\n",
    "  \n",
    "for _ in range(next_words): \n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "    token_list = pad_sequences( \n",
    "        [token_list], maxlen=max_sequence_len-1, padding='pre') \n",
    "    predicted_probs = model.predict(token_list) \n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)] \n",
    "    seed_text += \" \" + predicted_word \n",
    "  \n",
    "print(\"Next predicted words:\", seed_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a86ad-1f31-4bb5-8fd8-0b91de9599fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
